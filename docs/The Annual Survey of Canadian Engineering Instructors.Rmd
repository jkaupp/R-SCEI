---
bibliography: references.bib
output:
  html_document:
    fig_caption: yes
    theme: readable
---
```{r setup, echo=FALSE, message=FALSE}
source("/Users/Jake/ownCloud/Projects/R/Projects/R-SCEI/R/CEEA 2015 Analysis.R")
opts_chunk$set(echo = FALSE, dev="png", fig.retina = NULL, dpi=150, out.width = "800px", fig.align = "center")
```

#The Canadian Engineering Education Association Research Collaboration (CEEA-RC): Annual Survey of Canadian Engineering Instructors

* * *


*At the 2013 Canadian Engineering Education Association (CEEA) Annual General Meeting in Montreal an idea for a collaborative effort in engineering education research was proposed and steering committee was formed to guide the project. The goal of this project was to collectively strengthen our expertise in educational research methods, and introduce interested members to structured, scholarly efforts in engineering education.*

*The CEEA-RC proposal was to conduct an exploratory study of faculty attitudes toward teaching and about their inclination to collaborate for a variety of purposes. To measure these, the design, development and administration of a survey instrument was proposed. The survey was designed according to established principles, starting with the development of research questions, identification of a conceptual framework, and planning the survey items. A pilot version of the survey was created and refined through an internal delphi process.*

*This pilot version of the survey was presented at the 2014 CEEA annual meeting, to collect feedback from all collaborators and discuss future work.  Collaborator feedback was incorporated and the survey finalized and renamed to the "Survey of Canadian Engineering Instructors" or SCEI.  The next steps for the project were creating the online version of the SCEI, obtaining clearance with research ethics boards at participating institutions, implementing the survey and analyzing the results.*

*This paper will focus on the the design and development of the SCEI, from framework to final implemented version.  We will describe how the collaboration progressed, providing a narrative and insight into the collaborative research was enacted; highlighting methods and approaches used.  We will also discuss the implementation of the survey, and common issues and challenges encountered.  We will present data, highlight the preliminary findings and discuss the future of the survey and the research collaboration.*

**_Keywords: _** *Collaborative Research, Faculty Attitudes, Survey Design*

 * * *
 
#1. INTRODUCTION

The growing interest in engineering education research, as well as the continued community support for the Canadian Engineering Education Association (CEEA), led to the creation of the first collaborative research project at the 2013 annual meeting in Montreal.  The goals of the project are threefold:

1. Develop a community of people interested in engineering education research.				

2. Develop a survey of faculty attitudes toward teaching and about their inclination to collaborate with other instructors and developers, and toward professional development.

3. Collectively model discipline-based educational research methods as part of the process (identifying research method, conceptual framework, collaborating with educational researchers, using piloting with small groups, applying for ethics approval).

At the 2013 meeting CEEA members decided that the project would be guided by a steering committee, whose members possess a background in conducting rigorous research in engineering education, and could leverage existing initiatives and resources at their institutions to provide in-kind support for the collaborative project.   The steering committee would work closely with collaborators to ensure that the project captures the distinct and diverse nature of CEEA, yet the committee is responsible for the overall direction, implementation and dissemination of the research.

##2. DEVELOPMENT HISTORY

###2.1 Initial Planning & Phase 1

After the conclusion of the 2013 meeting a call went out to all CEEA members soliciting participation, and canvassing for general research questions or areas of interest.   The steering committee held initial meetings in the fall of 2013 to determine research questions, identify a conceptual frameworks, define a process for collaboration with CEEA members, how the steering committee would work, and the overall timeline for the project.   

The committee, after reviewing the feedback and community gathered from the 2013 meeting, developed the research questions to guide and focus the study, outlined below.

1. What are the current instructor attitudes about teaching and learning?

2. What are the current faculty attitudes about the role of the instructor and their duties in both the course and institution ?

3. What are the current faculty attitudes toward, and engagement in, professional development activities related to teaching?

Due to the large-scale nature of the project, the steering committee decided on using a modified Delphi process to elicit ideas from collaborators[@Brown:1968tz; @Sackman:1974tb].  This is a well-established method to elicit the opinions of experts and professionals, and has been used for survey and instrument development for educational research[@Facione:1990vn].  The modified process used in this project is outlined below:  

1. Elicit a ranked list of proposed questions or question topics from each collaborator. These proposals must address some aspect of the research questions. Collaborators rank their own ideas from highest to lowest priority. 

2. Steering committee will collect, organize, and improve consistency of the responses, and and return them to collaborators for feedback to ensure the intent has been captured. The steering committee will work with an education researcher to structure the questions in a way to assess validity of the instrument. 

3. Steering committee will send a list of clean proposals to collaborators to be ranked, and use rankings to finalize the instrument. 

The timeline for the project was agreed upon the steering committee, with the latter phases of the process allotted a greater amount of time to adjust for holidays and typical academic bottlenecks.

-------------------------------------------------------------------------------------------------------------
 Phase          Time             Task 
--------------- --------------- -----------------------------------------------------------------------------
Initial Phase    August-        Identify research questions, frameworks, timeline and scope 
                 October 2013

Phase 1          November 2013  Collaborator feedback on research questions, frameworks, 
                                timeline and provide ideas for survey items

Phase 2          December 2013- Steering committee works with experts to craft questions, 
                 February 2014  and develop survey in a flexible web-based survey platform.
                                Draft survey internally reviewed to reduce overlap and total number of items.

Phase 3          March-         Develop draft ethics application, LOI and consent forms for the project. 
                 June 2014      Present interim version of the survey at CEEA 2014 in Canmore.
              
Finalize Survey  June-          Incorporate final round of feedback.
Instrument       October 2014   Plan deployment strategy, obtain ethics approval from Queen’s.
                                Start ethics approval process at collaborating institutions

Deployment       November 2014- Obtain approval and deploy survey at participating institutions
                 TBD   

Analysis         April 2015-    Presentation of project at CEEA 2015 in Hamilton
                 TBD 
-------------------------------------------------------------------------------------------------------------

The initial phase provided collaborators with the research questions for the project along with the underlying framework for the second construct.  This framework is drawn from the work of John Biggs[@Biggs:2001vf], and addresses the development of quality teaching and learning in higher education, specifically focused on the views and beliefs of the instructor towards teaching and learning.  The steering committee felt that providing the more established framework would spark more discussion about items and potential revisions and will help and help shape the selection and development of the frameworks underlying the first and third research questions.   To streamline responses, the steering committee requested that each institution nominate a representative to collect all ranked feedback from the institution.   

###2.2 PHASE 2

The community feedback was reviewed by the steering committee and categorized according to which research question the comments or items addressed.  These responses were insightful and highly valuable, with the strong themes being represented in the responses providing direction to finalize the frameworks.  Suggestions for survey items were well distributed across the research questions and provided many insights into potential areas of analysis.   The steering committee then divided development, forming working groups to develop or refine the framework for each research question, and to develop items for the draft survey.  To ensure equitable work, the working group focusing on the second research question was also tasked with developing a module to collect demographic information of the survey population.

####2.2.1 SURVEY CONSTRUCTS

Each research question naturally formed a well-developed and rich constructs, and together they form an organizational layout and structure for the survey.  Presented below are the frameworks that were selected and refined for each research question.

**<u>Construct 1</u>**

This construct focuses on general perspectives and instructor attitudes about teaching and learning,

**Research Question: **What are the current instructor attitudes about teaching and learning?

**Framework:**  This construct draws upon the theories underlying the teaching perspectives inventory (TPI)[@Pratt:2000vd].  The TPI focuses on five conceptual areas or modes of instruction: Transmission, Apprenticeship, Developmental, Nurturing, and Social Reform. The TPI contains questions about learning, motivation, the goals of education, their role as a teacher, the nature of the learners they taught, and the influence of context on their teaching.  

**<u>Construct 2</u>**

This construct focuses on the role of the instructor, their conceptions of effective teaching and learning, the roles of student and instructor, and reflective practise. 

**Research Question:** What are the current faculty attitudes about the role of the instructor and their duties in both the course and institution ?

**Framework:**  The underlying framework for this construct was developed by John Biggs, and identifies three common attitudes about teaching in higher education[@Biggs:2001vf].

**Level 1. Focus: What the student is.**

Teachers using a Level 1 theory are struck by student differences, as most beginning teachers are. They see students as easily teachable, or not. They assume a teacher-centred, transmission model of teaching. The teacher is the guardian of knowledge, whose responsibility is to know the content well, and to expound it clearly. It is then up to the student to attend lectures, to listen carefully, to take notes, to read the recommended readings, and so on. Differences in learning outcome occur because students differ in their ability, their motivation, their background, and so on. Thus, when teaching is not effective, it is seen as the students’ fault. Level 1 theory does not promote reflection, whereby the teacher asks the key generative question that all expert practitioners ask: "Is my present practice the best way of doing this?"

**Level 2. Focus: What the teacher does**.

The Level 2 theory is also based on transmission, but of complex knowledge structures, which require skill in presenting to students, so that learning outcomes are now seen as more a function of how skillful the teacher is. Level 2 theory emphasizes what the teacher does: forward planning, good management skills, an armoury of teaching competencies, ability to use IT, and so on. Retrospective QA uses Level 2 theorising when it talks about teaching competencies, and distinguished teacher awards (see below), as if focusing on what teachers do is in itself an index of student learning. In Level 2, means becomes ends.

**Level 3. Focus: What the student does.**

Level 3 theory focuses not on teachers, but on teaching that leads to learning. Expert teaching in this sense certainly includes mastery of teaching techniques, but unless the appropriate learning takes place, it is an empty display. Tyler, fifty years ago, said that learning "takes place through the active behavior of the student: it is what he does that he learns, not what the teacher does" (Tyler 1949, p. 63). Likewise Shuell: If students are to learn desired outcomes in a reasonably effective manner, then the teacher’s fundamental task is to get students to engage in learning activities that are likely to result in their achieving those outcomes (Shuell 1986, p. 429).

**<u>Construct 3</u>**

These construct focuses on identifying what  resources are available to  instructors, what resources they would participate in, and potential barriers for participation in professional development related to teaching.

**Research Question:** What are the current faculty attitudes toward, and engagement in, professional development activities related to teaching?

**Framework:**  Construct 3 is based on a conceptual framework for professional development adapted by one by Amundsen et al. (2005) that categorizes four main focal areas in faculty development: skills focus, method focus, process focus, and discipline focus[@Amundsen:2005wb].  The Amundsen framework was adapted for a disciplinary focus to the following framework for professional development activities. 

| Categories                                                                      | Disciplinary Focus                                                                                                                                                                                                                                                                                                                                                           | Multidisciplinary Focus                                                                                                                                                                                                                                                                         |
|---------------------------------------------------------------------------------|------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
| Skills (presentations, discussion facilitation, learning technology)            | Training on using engineering hardware/software in courses.  Personal reading on using engineering hardware/software in courses.                                                                                                                                                                                                                                             | Training on using general educational software/hardware (learning management systems, clickers, etc.)  Personal reading on general education hardware/software in courses (learning management systems, clickers, etc.)  Training on organization, presentations, writing on a blackboard, etc. |
| Teaching methods (project-based learning, case studies, active learning, etc.)  | Workshops/training on teaching methods specific to engineering (education sessions at disciplinary conferences, workshops on teaching design, engineering labs, etc.)  Personal reading on teaching methods specific to engineering (education sessions at disciplinary conferences, workshops on teaching design, engineering labs, etc.)                                   | Workshops/training on general teaching methods (active learning, service learning, collaborative learning, etc.)  Personal reading on general teaching methods (active learning, service learning, collaborative learning, etc.)                                                                |
| Processes and critical analysis                                                 | Workshops/sessions on teaching and learning processes specific to engineering (course redevelopment workshops, curriculum design, assessment, graduate attributes)  Facilitated sessions on working collaboratively as an engineering department on curriculum design, assessment, etc.  Broad informal holistic discussions on teaching and learning issues with colleagues | Workshop/session on general teaching and learning processes (constructive alignment, curriculum design, assessment, etc.) Facilitated sessions on curriculum design, assessment, learning science, etc.                                                                                         |
| Personal scholarship                                                            | Presenting and seeking feedback to engineering colleagues on teaching and learning innovations  Scholarly work related to engineering education                                                                                                                                                                                                                              |                                                                                                                                                                                                                                                                                                 |

##2.3 Phase 3


###2.3.1 DRAFT SURVEY

Once the frameworks were finalized, each working group created draft items for their respective construct.  These items were then reviewed by the steering committee alongside educational experts in survey creation.  The intent of the internal review was the streamline the draft survey prior to presenting it to collaborators at the CEEA 2014 meeting. This resulted in the removal and merging items, reallocating items to another construct, or outright removal of items that were deemed unnecessary.   The survey was then created in a web-based platform, FluidSurveys, as this was the survey platform used by the host institution, Queen’s University.   This draft survey was packaged with a brief synopsis of the research collaboration thus far (Goals, Development, Research Questions and Frameworks) and was presented to collaborators at the 2014 annual meeting.  It was at this point that the instrument was named the "Survey of Canadian Engineering Instructors" or SCEI.

###2.2.2 ETHICS APPROVAL

In anticipation of questions from the community regarding ethics approval for the project, a draft application was created along with a combined letter of information and consent form.  The draft application was completed by the steering committee and would be submitted to the General Research Ethics Board (GREB) at Queen’s University upon the survey being finalized.  Additionally, the steering committee consulted with the director of the Queen’s Research Ethics Board (REB) for the proposed workflow for conducting collaborative national level research on this scale, shown below:

1. Steering committee applies to Queen’s GREB for approval for the national study

2. Upon obtaining approval from Queen’s apply to participating institutions individual REB boards, providing them the Queen’s REB Approval, and all required supporting information

3. Once institutional approval is obtained, research can begin.

The merged letter of information and consent form was drafted according to Tri-Council Policy Statement (TCPS) and Queen's University guidelines.  These documents were prepared to give collaborators at other institutions an idea of what type of information would be required in their own future applications to their institutional REBs.  In order to meet the needs of specific institutional REBs, these draft approval documents would most likely be ammended.  

##2.4 FINALIZING THE SURVEY INSTRUMENT

Immediate collaborator and community feedback was collected from the session at the conclusion of the 2014 CEEA annual meeting.  Collaborators were also encouraged to take some time to consider and reflect upon the draft version of the survey, and provide their feedback to the steering committee by the end of the summer.

The majority of the feedback collected from the conference session was incredibly positiv and largely constituted corrections to tenses and phrasing about the instrument.  This resulted in some response options being changed, as collaborators thought some of the response options could be more clearly worded.  There were also some minor changes made to the demographics section of the survey, to provide better options and avenues for future analysis.  This feedback, along with the combined LOI and consent form was incorporated into a web-based survey in FluidSurveys.

###2.4.1 ETHICS APPROVAL

Upon completion of the final version of the survey, a formal application for the project was submitted to Queen's University.  The project received Queen's Ethics Approval September 10, 2014, nearly a full year after the project had started.  Due to the start of the academic year, the call for implementing the survey went out to collaborators on October 28th.  This message presented two options to deploy the survey, with each requiring that individual institutions obtain approval from their REB prior to starting research.  In order to support those new to this aspect of research, and to streamline individual applications, a member of the steering committee would work with collaborators to complete their respective ethics applications.

1. **Institutionally Hosted**: Institutions host and run the survey themsleves. They collect their survey data, then share a copy of with the steering committee for use in the aggregate national dataset.  

2. **Queen's Hosted**: Queen's hosts the survey, with each participating institution having it's own separate collector with unique variables to highlight differences in personnel and variations in addressing REB requests. Institutions are then provided a copy of their data for their own analysis.

The typical workflow for deployment combined choosing deployment options, ethics approval, getting approval from the Dean of Engineering, contacting faculty, and surveying.  For clarity, it is shown below:

1.  Select deployment strategy
2.  Contact ethics board to determine application type
3.  Work with steering committee member to obtain ethics approval
4.  Contact the Dean of Engineering to inform them and obtain approval
5.  Contact the faculty members to invite them to participate
6.  Schedule a reminder to participate at a later date

Institutional deployments started in late November of 2014, working towards obtaining institutional ethics approval.  The survey started to go live in January 2015.  The steering committee decided to let each institution set its own timeline, as many collaborators wished to coincide the deployment of the survey with specific events, or to adapt to unforeseen consequences.  

##2.5 CURRENT STATUS

As of writing this paper, there have been 10 different institutions that have been involved with the project thus far. The survey has launched and completed at 8 of those institutions, one of those institutions is waiting to contact faculty at the conclusion of the academic year, and one institution is just beginning the ethics approval process.  

The steering committee expects that following the presentation of this paper, there may be more institutions wanting to participate in the project.  To accomodate those yet to survey their faculty, and to accomodate potential newcomers we are keeping the project open until September 2015, which coincides with the ethics renewal for the project.

#3. RESULTS 

The results presented in this paper are a presentation of summary statistics and general observations regarding constructs and items.  This interim report utilizes all survey responses collected up until April 15th, 2015.  A more thorough analysis of the survey and the results will be conducted in the future, upon the conclusion of data collection.

##3.1 DEMOGRAPHICS

The response for the survey was quite positive, illustrated below in table 1.

```{r Summary, fig.cap="Summary Statistics"}
set.caption('1: Summary response statistics') 
survey_data %>% 
  summarise(Responses=n_distinct(key),
            Institutions=n_distinct(collector),
            "Avg.Time (mins)"=mean(time)) %>% 
  pander
```

Out of 22 institutions that expressed interest in the research collaboration, only 8 have implemented the survey.  At the April 15th cutoff point, two instituions were in the process of approval or implemenetation. If faculty counts were obtained for each institution, a true response rate could be genreated.  Respondents took an average time of 15.44 minutes to complete the survey, which is under the anecdotal 20 minute 'rule of thumb' to maintain survey engagement.

The distribution of instructor classification, shown below in table 2, illustrates that respondents were mostly classified as a traditional professor.  Those who responded "other", show a mix of emeritus professors, adminstrators and specialized staff shown in table 3.

```{r Q1_Table_2, fig.cap="Instructor Academic Title"}
set.caption('2: Distribution of Instructor Classification') 
survey_data %>% 
  count("Academic Title"=position) %>% 
  pander
```

```{r Q1b_Table_3, fig.cap="Other Instructor Roles"}
set.caption('3: Other Roles') 
survey_data %>% 
  count("Academic Title"=position_other) %>%
  na.omit %>% 
  pander
```

The majority of the population was in the 7-15 years, with a relatively even distribution in the remaining categories, shown below in table 4. 
```{r Q2_Table_4, fig.cap="Teaching Experience"}
set.caption('4: Teaching Experience') 
survey_data %>% 
 count("Teaching Experience"=teaching_duration) %>% 
 pander
```

The majority of the population was focused largely on undergraduate teaching, reporting that teaching occupies approximately 21-60% of faculty work time, illustrated below in table 5.

```{r Q3_4_Table_5, fig.cap="Teaching focus and division of work time"}
u.vs.g <- survey_data %>% 
  count(undergrad_vs_grad) %>% 
  set_names(c('Category','# of Responses:Undergrad vs Graduate Teaching'))

avg.t.time <- survey_data %>% 
  count(avg_teaching_time) %>% 
  set_names(c('Category','# of Responses: Average teaching time per year'))

set.caption('5: Teaching focus and Division of Work Time') 
inner_join(u.vs.g,avg.t.time,by='Category') %>% 
  set_names(c('Percentage','# of Responses:Undergrad vs Graduate Teaching','# of Responses: Average teaching time per year')) %>% 
  pander
```

## 3.2 CONSTRUCT 1: GENERAL PERSPECTIVES AND ATTIUDES

The majority of survey respondents would classify themselves as enjoying teaching to a great or fairly great extent, as illustrated in figure 1 below.

```{r Q5_Figure_1, fig.cap="Figure 1"}
survey_data %>% 
  select(c(10)) %>% 
  plyr::rename(c(enjoyment = paste("Extent of enjoyment"))) %>% 
  likert %>% 
  plot(wrap=20) %>% 
  + ggtitle(str_wrap('Engineering instructors enjoying of teaching',50)) %>% 
  + theme(legend.text= element_text(size=9))
```

The top teaching goals of survey respondents were transmission of information (76%, ranked by top priority), adopting a learner-centered development of understanding and facilitating construction of meaning (60%).  Nurturing students to reach their personal potential, was third (35%) with a relatively even split.  The lowest priorities were social change through education of the next generation of engineers (73% ranked lowest priority), apprenticeship by socializing students into the practice of engineering (55%).

```{r Q6_Figure_2, fig.cap="Figure 2"}
survey_data %>%  
  select(c(11:15)) %>% 
  plyr::rename(c(goals_transmission = "Transmission",
           goals_apprenticeship = "Apprenticeship",
           goals_learner_centered = "Learner Centered",
           goals_nuturing = "Nurturing",
           goals_social_change = "Social Change")) %>% 
  likert() %>% 
  plot(wrap=20) %>% 
  + ggtitle(str_wrap('Reported teaching goals of engineering instructors',50)) %>% 
  + theme(legend.text= element_text(size=9))
```

The primary influences of survey respondents to make changes in their teaching are oultined below in figure 3.  Personal obersvation from previous teaching (92%) and course eveluations (63%) were the mostinfluential, with professional development (31%), literature on teaching and learning (29%) and input from colleagues being indicated as being less influential.

```{r Q7_Figure_3, fig.cap="Figure 3"}
survey_data %>%  
  select(c(18:22)) %>% 
  plyr::rename(c(changes_personal_observation = "Personal Observation",
           changes_colleague_input = "Colleague Input",
           changes_course_evaluations = "Course Evaluations",
           changes_literature = "Literature",
           changes_pd = "Professional Development")) %>% 
  likert() %>% 
  plot(wrap=20) %>% 
  + ggtitle(str_wrap('Instuctors reported influences to change teaching',50)) %>% 
  + theme(legend.text= element_text(size=9))
```

Survey respondents indicated that, when they are dissatisfied with student learning in their course, they believe that the top three reasons are: student workload, inadequate learning skills and underprepared or unmotivated students.  There were very few responses that indicated teaching, resources or curriculum as a potential factor. 

```{r Q8_Table_6, fig.cap="Common problems with student learning"}
q.8 <- survey_data %>% 
  select(c(25:37)) %>% 
  colSums(na.rm=TRUE) %>% 
  data.frame(names(.),.)

rownames(q.8) <- NULL

q.8 %<>%
  setNames(c("Reason","Frequency"))

q.8$Reason %<>%
 str_replace_all("_"," ") %>% 
 stri_trans_totitle()

set.caption('6: Instructor perceptions of common problems when dissatisfied with student learning') 
q.8 %>% 
  pander
```

## 3.3 CONSTRUCT 2: ATTITUDES ABOUT THE ROLE AND DUTIES OF AN INSTRUCTOR

When asked a series of questions to determine the roles and duties of instructors, survey respondents overwhelmingly indicated that the primary responsibility of an instructor was to motivate students to learn, provide them with a clear explanation of what it is they are expected learn, and provide learning opportunities in which they can deeply engage (61%). The other two categories were a bit difficult to discern, with content knowlegde and articulation and knowledge and application of best teaching practices being more evenly split in rankings, as illustrated below in figure 4.

```{r Q10_Figure_4, fig.cap="Figure 4"}
survey_data %>% 
  select(c(39:41)) %>% 
  plyr::rename(c(instructor_teaching_practises = "Know and follow best teaching practices in order to convey to students the important concepts and complex understandings of the content",
           instructor_content = "Know the content well and be able to clearly articulate it",
           instructor_motivate_and_provide = "Motivate students to learn, provide a clear explanation of what they are expected learn, and provide learning opportunities in which they can deeply engage")) %>% 
  likert() %>% 
  plot(wrap=20) %>% 
  + ggtitle(str_wrap('Perception of instructor responsibility in the teaching and learning process',50)) %>% 
  + theme(legend.text = element_text(size=9))
```

Instructor perceptions of student responsibilites in teaching and learning show that the majority of survey respondents agree that the student is responsible for knowledge and motivation (54%), as well as 'doing the work' (48%).  Nearly all respondents indicated that students share the responsibility in teaching and learning, ranking the statement "If the student has the ability to do the work, he or she doesn't really need to take on any added responsibility" last (Figure 5).

```{r Q11_Figure_5, fig.cap="Figure 5"}
survey_data %>% 
  select(c(42:44)) %>% 
  plyr::rename(c(student_ability_no_responsbility = "If the student has the ability to do the work, he or she doesn't really need to take on any added responsibility",
           student_responsible_background_motivation = "Appropriate background knowledge & to develop his or her own motivation to learn about the subject",
           student_attend_classes = "Attend classes, be attentive, take good notes, do the readings & assignments, and study")) %>% 
  likert %>% 
  plot(wrap=20) %>% 
  + ggtitle(str_wrap('Instructor perception of student responsibility in the teaching and learning process',50)) %>% 
  + theme(legend.text= element_text(size=9))
```

Interestingly, instructor perceptions of how to positively influence student success identify a teacher focused theme, illustrated in figure 6.  Effective presentation, classroom management and engaging learning opportunities was ranked as most important (56%), with the other two choices of content mastery and clear articulation adnd teaching and assessment methods supporting outcomes being virtually indistinguishable.

```{r Q12_Figure_6, fig.cap="Figure 6"}
survey_data %>% 
  select(c(45:47)) %>% 
  plyr::rename(c(methods_supporting_outcomes = "Use teaching and assessment methods that support clearly stated learning outcomes",
           presentation_opportunity_management = "Speak effectively, provide structured and engaging learning opportunities, and manage the classroom effectively",
           content_clarity = "Know the subject very well and explain it very clearly")) %>% 
  likert %>% 
  plot(wrap=20) %>% 
  + ggtitle(str_wrap('Perception of how instructors can positively influence student success',50)) %>% 
  + theme(legend.text= element_text(size=9))
```

Nearly all respondents identified themselves as both a knowledge and teaching expert in their area of specialization (Figure 7).

```{r Q14_15_Figure_7,fig.cap="Figure 7"}
survey_data %>% 
  select(c(49:50)) %>% 
  plyr::rename(c(knowledge_expert = "I think of myself as a knowledge expert in my area of specialization",
           teaching_expert = "I think of myself as an expert in teaching my area of specialization")) %>% 
  likert %>% 
  plot(wrap=20) %>% 
  + ggtitle(str_wrap('Perception of instructor expertise in disciplinary content knowledge and teaching',50)) %>% 
  + theme(legend.text= element_text(size=9))
```

Nearly all respondents responded that they primarily use student course and teaching evaluations to improve delivery (68 responses).  Other uses of student evaluations were spread relatively equal across other categories, with only a very small minority (4 responses) not reviewing course evaluations.

```{r Q16_Table_7, fig.cap="Table 7"}
q.16 <- survey_data %>% 
  select(c(51)) %>% 
  table() %>% 
  as.data.frame %>% 
  setNames(c("Action","Frequency"))

set.caption('7: Perception of important uses of student course & teaching evaluations') 
q.16 %>% 
  pander
```

## 3.4 CONSTRUCT 3: ATTITUDE TOWARDS AND ENGAGEMENT IN PROFESSIONAL DEVELOPMENT

Nearlly all categories for professiona development were ranked as important for engineering educators (Figure 8). Teaching processes (curriculum development, program improvement), teaching skills and teaching assessment methods were virtually the same, but scholarship was the item that produced the most diversity in responses.  

```{r Q18_Figure_8, fig.cap="Figure 8"}
survey_data %>% 
  select(c(53:56)) %>% 
  plyr::rename(c(teaching_skills_development = "Teaching Skills Development",
           teaching_assessment_methods = "Teaching Assessment Methods",
           teaching_processes = "Teaching Processes",
           scholarship = "Scholarhip")) %>% 
  likert %>% 
  plot(wrap=20) %>% 
  + ggtitle(str_wrap('Perception of the importance of select professional development activites',50)) %>% 
  + theme(legend.text= element_text(size=9))
```

In light of the previous question, participation in professional development activities echoes the ranking of importance. What is surprising is that this trend is reversed in the awarness of support for professional development activites, with the awareness of support for scholarship being the highest rated (68%).

```{r Q19_Figure_9, fig.cap="Figure 9"}
survey_data %>% 
  select(c(57:60)) %>% 
  plyr::rename(c(support_teaching_skills_development = "Support for Teaching Skills Development",
           support_teaching_assessment_methods = "Support for Teaching Assessment Methods",
           support_teaching_processes = "Support for Teaching Processes",
           support_scholarship = "Support for Scholarhip")) %>% 
  likert %>% 
  plot(wrap=20) %>% 
  + ggtitle(str_wrap('Instructor awarness and participation in professional development activities',50)) %>% 
  + theme(legend.text= element_text(size=9))
```

Within the past five years respondent participation in teaching professional development was mainly participation in 1) small 1-2 hour seminars (116 responses), 2) independent learning (112 responses) and 3) 3 hour to full day workshops on teaching (90 responses). 

```{r Q20_Table_8}
q.20 <- survey_data %>% 
  select(c(61:70)) %>% 
  colSums(na.rm = TRUE) %>% 
  data.frame(names(.),.)

rownames(q.20) <- NULL

q.20 %<>%
  setNames(c("Activity","Frequency"))

q.20$Activity <- c("Attended a seminar on teaching (1-2 hours of professional development)",
                   "Participated in a workshop on teaching  (3 hours to a full day of professional development)",
                  "Participated in a multi-day workshop on teaching (several day professional development activity)",
                  "Participated in conference related to education (either disciplinary or not)",
                  "Learning independently through reading, etc.",
                  "Led workshops focusing on teaching and learning development",
                  "Internal university funding to support course or program development",
                  "External funding to support course or program development",
                  "Internal university grants supporting educational research",
                  "External grants supporting educational research")

set.caption('8: Instructor participation in faculty development activities in the past 5 years') 
q.20 %>% 
  pander
```

The raking of obstacles to participation in professional development activities are illustrated below in figure 10.  The two greatest obstacles indicated by respondents were workload (91%) and the timing of the event (84%).  The two least significant obstacles were the lack of access to funding (47%), and the lack of access to expertise (85%).

```{r Q21_Figure_10, fig.cap="Figure 10"}
survey_data %>% 
  select(c(71:79)) %>% 
  plyr::rename(c(obs_timing = "Timing of event",
                 obs_availability = "Availability of event",
                 obs_location = "Location of event",
                 obs_awareness = "Awareness of event",
                 obs_relevance = "Relevance of event",
                 obs_workload = "Workload",
                 obs_lack_funding = "Lack of funding opportunities",
                 obs_lack_expertise = "Lack of access to expertise",
                 obs_specificity = "Specificity of the event")) %>% 
  likert() %>% 
  plot(wrap=20) %>% 
  + ggtitle(str_wrap('Instructor perception of obstacles to professional development',50)) %>% 
  + theme(legend.text= element_text(size=9))
```

Compared to question 20, the responses in this item illustrated that the majority of respondents participation in professional development is skewed towards non-teaching focused activities.  There were 89 responses that responded that 0-40% of their professional development activities in the past 5 years were teaching-focused, which accounts for approximately 54% of the population.

```{r Q22_Figure_11, fig.cap="Figure 11"}
set.caption('9: Percentage of teaching-focused professional development activities attended in the past five years ') 
survey_data %>% 
  select(c(80)) %>% 
  table() %>% 
  as.data.frame %>% 
  setNames(c("Percentage","Frequency")) %>% 
  pander
```

Nearly all respondents indicated that professional development as training for new faculty was needed across all areas.  Yet the order of these areas differs from the question 19.  The perception is that new faculty should be provided training in assessment and teaching skills first, with teaching processes and scholarship being of lesser importance.  
```{r Q23a_Figure_12, fig.cap="Figure 12"}
survey_data %>% 
  select(c(81:84)) %>% 
  plyr::rename(c(nf_teaching_skills = "Teaching skills development",
                 nf_teaching_assessment_methods = "Teaching of assessment methods",
                 nf_teaching_processes = "Teaching processes",
                 nf_scholarship = "Scholarship")) %>% 
  likert() %>% 
  plot(wrap=20) %>% 
  + ggtitle(str_wrap('Instructor perception of needed areas of professional development as training for new faculty',50)) %>% 
  + theme(legend.text= element_text(size=9))
```

This trend also held true in the second part of the question pertaining to professional development as continuing education for experienced faculty.  Interestingly, there was a lower level of agreement regarding this question compared to the responses for training for new faculty. 
```{r Q23b_Figure_13, fig.cap="Figure 13"}
survey_data %>% 
  select(c(85:88)) %>% 
  plyr::rename(c(ce_teaching_skills = "Teaching skills development",
                 ce_teaching_assessment_methods = "Teaching of assessment methods",
                 ce_teaching_processes = "Teaching processes",
                 ce_scholarship = "Scholarship")) %>% 
  likert() %>% 
  plot(wrap=20) %>% 
  + ggtitle(str_wrap('Instructor perception of needed areas of professional development as continuing education for for experienced faculty',50)) %>% 
  + theme(legend.text= element_text(size=9))
```

Survey respondents were split on whether or not professional development activities were taken into account during annual performance review. The majority (70 responses) indicated that, yes, professional development activities were taken into account.  A large number of respondents were unsure if professional development was used (64) and very few answered that, no, professional development activities were not taken into account.  This highlights the potential need for futher clarification of the benefit and impact of professional development activities beyond the individual.

```{r Q24_Table_10}
set.caption('10: Instructor perception of institutions taking professional development activities into account during annual performance evaluations ') 
survey_data %>% 
  select(c(89)) %>% 
  table() %>% 
  as.data.frame %>% 
  setNames(c("Percentage","Frequency")) %>% 
  pander
```

# 4. CONCLUSIONS

The results from this initial sample of responses has highlight some interesting avenues for analysis and future investigation. The initial results were from the scale items alone, and did not include any of the open-response items.  The reason for this was time-based, as qualiative analysis of these responses takes considerable time.

The next steps for the research collaboration are to complete the implementation of the survey at the remaining institutions, and include any additional institutions that have yet to participate.  Once data collection has concluded, a paper is planned to address the development and validation of the instrument along with the report on the complete national dataset. More immediate concerns are the timeline for dissemination of instutional data and the aggregate national dataset, and the long term management, collection and reporting on the data.  

Even in its infancy, this instrument and approach has garnered attention from colleagues outside of engineering and the steering committee has received positive feedback about the impact of this project and its importance.  This highlights a need for the continued use, development and improvement of the instrument, as well as a effort to move the CEEA research collaboration into the next phase.

In the interest of conducting open and collaborative research, nearly all collection, analysis, writing and report generation was done using open-source or readily available technologies.  These include Google Drive, R, RStudio, Github, Atom.  This report was written in rMarkdown in RStudio, typset using Knitr and hosted at Gitub.  You can view this report, the code that created and associated analysis at [http://github.com/jkaupp/R-SCEI](http://github.com/jkaupp/R-SCEI)

# 5. ACKNOWLEDGEMENTS

The steering committe would like to thank the CEEA for its support and assistance in conducting this project.

# References