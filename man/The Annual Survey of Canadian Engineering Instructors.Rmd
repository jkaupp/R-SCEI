---
output: html_document
---
```{r setup, echo=FALSE, message=FALSE}
source("/Users/Jake/ownCloud/Projects/R/Projects/R-SCEI/R/CEEA 2015 Analysis.R")
opts_chunk$set(echo = FALSE, dev="png", fig.retina = NULL, dpi=150, out.width = "487px", fig.align = "center")
```

#The Canadian Engineering Education Association Research Collaboration (CEEA-RC): Annual Survey of Canadian Engineering Instructors

* * *


*At the 2013 Canadian Engineering Education Association (CEEA) Annual General Meeting in Montreal an idea for a collaborative effort in engineering education research was proposed and steering committee was formed to guide the project. The goal of this project was to collectively strengthen our expertise in educational research methods, and introduce interested members to structured, scholarly efforts in engineering education.*

*The CEEA-RC proposal was to conduct an exploratory study of faculty attitudes toward teaching and about their inclination to collaborate for a variety of purposes. To measure these, the design, development and administration of a survey instrument was proposed. The survey was designed according to established principles, starting with the development of research questions, identification of a conceptual framework, and planning the survey items. A pilot version of the survey was created and refined through an internal delphi process.*

*This pilot version of the survey was presented at the 2014 CEEA annual meeting, to collect feedback from all collaborators and discuss future work.  Collaborator feedback was incorporated and the survey finalized and renamed to the "Survey of Canadian Engineering Instructors" or SCEI.  The next steps for the project were creating the online version of the SCEI, obtaining clearance with research ethics boards at participating institutions, implementing the survey and analyzing the results.*

*This paper will focus on the the design and development of the SCEI, from framework to final implemented version.  We will describe how the collaboration progressed, providing a narrative and insight into the collaborative research was enacted; highlighting methods and approaches used.  We will also discuss the implementation of the survey, and common issues and challenges encountered.  We will present data, highlight the preliminary findings and discuss the future of the survey and the research collaboration.*

**_Keywords: _** *Collaborative Research, Faculty Attitudes, Survey Design*
 * * *
 
##1. INTRODUCTION

The growing interest in engineering education research, as well as the continued community support for the Canadian Engineering Education Association (CEEA), led to the creation of the first collaborative research project at the 2013 annual meeting in Montreal.  The goals of the project are threefold:

1. Develop a community of people interested in engineering education research.				

2. Develop a survey of faculty attitudes toward teaching and about their inclination to collaborate with other instructors and developers, and toward professional development.

3. Collectively model discipline-based educational research methods as part of the process (identifying research method, conceptual framework, collaborating with educational researchers, using piloting with small groups, applying for ethics approval).

At the 2013 meeting CEEA members decided that the project would be guided by a steering committee, whose members possess a background in conducting rigorous research in engineering education, and could leverage existing initiatives and resources at their institutions to provide in-kind support for the collaborative project.   The steering committee would work closely with collaborators to ensure that the project captures the distinct and diverse nature of CEEA, yet the committee is responsible for the overall direction, implementation and dissemination of the research.

##2. DEVELOPMENT HISTORY

###2.1 Initial Planning & Phase 1

After the conclusion of the 2013 meeting a call went out to all CEEA members soliciting participation, and canvassing for general research questions or areas of interest.   The steering committee held initial meetings in the fall of 2013 to determine research questions, identify a conceptual frameworks, define a process for collaboration with CEEA members, how the steering committee would work, and the overall timeline for the project.   

The committee, after reviewing the feedback and community gathered from the 2013 meeting, developed the research questions to guide and focus the study, outlined below.

1. What are the current instructor attitudes about teaching and learning?

2. What are the current faculty attitudes about the role of the instructor and their duties in both the course and institution ?

3. What are the current faculty attitudes toward, and engagement in, professional development activities related to teaching?

Due to the large-scale nature of the project, the steering committee decided on using a modified Delphi process to elicit ideas from collaborators{Brown:1968tz}.  This is a well-established method to elicit the opinions of experts and professionals, and has been used for survey and instrument development for educational research{Facione:1990vn}.  The modified process used in this project is outlined below:  

1. Elicit a ranked list of proposed questions or question topics from each collaborator. These proposals must address some aspect of the research questions. Collaborators rank their own ideas from highest to lowest priority. 

2. Steering committee will collect, organize, and improve consistency of the responses, and and return them to collaborators for feedback to ensure the intent has been captured. The steering committee will work with an education researcher to structure the questions in a way to assess validity of the instrument. 

3. Steering committee will send a list of clean proposals to collaborators to be ranked, and use rankings to finalize the instrument. 

The timeline for the project was agreed upon the steering committee, with the latter phases of the process allotted a greater amount of time to adjust for holidays and typical academic bottlenecks.

-------------------------------------------------------------------------------------------------------------
 Phase          Time             Task 
--------------- --------------- -----------------------------------------------------------------------------
Initial Phase    August-        Identify research questions, frameworks, timeline and scope 
                 October 2013

Phase 1          November 2013  Collaborator feedback on research questions, frameworks, 
                                timeline and provide ideas for survey items

Phase 2          December 2013- Steering committee works with experts to craft questions, 
                 February 2014  and develop survey in a flexible web-based survey platform.
                                Draft survey internally reviewed to reduce overlap and total number of items.

Phase 3          March-         Develop draft ethics application, LOI and consent forms for the project. 
                 June 2014      Present interim version of the survey at CEEA 2014 in Canmore.
              
Finalize Survey  June-          Incorporate final round of feedback.
Instrument       October 2014   Plan deployment strategy, obtain ethics approval from Queenâ€™s.
                                Start ethics approval process at collaborating institutions

Deployment       November 2014- Obtain approval and deploy survey at participating institutions
                 TBD   

Analysis         April 2015-    Presentation of project at CEEA 2015 in Hamilton
                 TBD 
-------------------------------------------------------------------------------------------------------------

The initial phase provided collaborators with the research questions for the project along with the underlying framework for the second construct.  This framework is drawn from the work of John Biggs{Biggs:2001vf}, and addresses the development of quality teaching and learning in higher education, specifically focused on the views and beliefs of the instructor towards teaching and learning.  The steering committee felt that providing the more established framework would spark more discussion about items and potential revisions and will help and help shape the selection and development of the frameworks underlying the first and third research questions.   To streamline responses, the steering committee requested that each institution nominate a representative to collect all ranked feedback from the institution.   

###2.2 PHASE 2

The community feedback was reviewed by the steering committee and categorized according to which research question the comments or items addressed.  These responses were insightful and highly valuable, with the strong themes being represented in the responses providing direction to finalize the frameworks.  Suggestions for survey items were well distributed across the research questions and provided many insights into potential areas of analysis.   The steering committee then divided development, forming working groups to develop or refine the framework for each research question, and to develop items for the draft survey.   To ensure equitable work, the working group focusing on the second research question was also tasked with developing a module to collect demographic information of the survey population.

####2.2.1 SURVEY CONSTRUCTS

Each research question naturally formed a well-developed and rich constructs, and together they form an organizational layout and structure for the survey.  Presented below are the frameworks that were selected and refined for each research question.

**<u>Construct 1</u>**

This construct focuses on general perspectives and instructor attitudes about teaching and learning,

**Research Question: **What are the current instructor attitudes about teaching and learning?

**Framework:**  This construct draws upon the theories underlying the teaching perspectives inventory (TPI){Pratt:2001wm}.  The TPI focuses on five conceptual areas or modes of instruction: Transmission, Apprenticeship, Developmental, Nurturing, and Social Reform. The TPI contains questions about learning, motivation, the goals of education, their role as a teacher, the nature of the learners they taught, and the influence of context on their teaching.  

**<u>Construct 2</u>**

This construct focuses on the role of the instructor, their conceptions of effective teaching and learning, the roles of student and instructor, and reflective practise. 

**Research Question:** What are the current faculty attitudes about the role of the instructor and their duties in both the course and institution ?

**Framework:**  The underlying framework for this construct was developed by John Biggs, and identifies three common attitudes about teaching in higher education{Biggs:2001vf}.

**Level 1. Focus: What the student is.**

Teachers using a Level 1 theory are struck by student differences, as most beginning teachers are. They see students as easily teachable, or not. They assume a teacher-centred, transmission model of teaching. The teacher is the guardian of knowledge, whose responsibility is to know the content well, and to expound it clearly. It is then up to the student to attend lectures, to listen carefully, to take notes, to read the recommended readings, and so on. Differences in learning outcome occur because students differ in their ability, their motivation, their background, and so on. Thus, when teaching is not effective, it is seen as the studentsâ€™ fault. Level 1 theory does not promote reflection, whereby the teacher asks the key generative question that all expert practitioners ask: "Is my present practice the best way of doing this?"

**Level 2. Focus: What the teacher does**.

The Level 2 theory is also based on transmission, but of complex knowledge structures, which require skill in presenting to students, so that learning outcomes are now seen as more a function of how skillful the teacher is. Level 2 theory emphasizes what the teacher does: forward planning, good management skills, an armoury of teaching competencies, ability to use IT, and so on. Retrospective QA uses Level 2 theorising when it talks about teaching competencies, and distinguished teacher awards (see below), as if focusing on what teachers do is in itself an index of student learning. In Level 2, means becomes ends.

**Level 3. Focus: What the student does.**

Level 3 theory focuses not on teachers, but on teaching that leads to learning. Expert teaching in this sense certainly includes mastery of teaching techniques, but unless the appropriate learning takes place, it is an empty display. Tyler, fifty years ago, said that learning "takes place through the active behavior of the student: it is what he does that he learns, not what the teacher does" (Tyler 1949, p. 63). Likewise Shuell: If students are to learn desired outcomes in a reasonably effective manner, then the teacherâ€™s fundamental task is to get students to engage in learning activities that are likely to result in their achieving those outcomes (Shuell 1986, p. 429).

**<u>Construct 3</u>**

These construct focuses on identifying what  resources are available to  instructors, what resources they would participate in, and potential barriers for participation in professional development related to teaching.

**Research Question:** What are the current faculty attitudes toward, and engagement in, professional development activities related to teaching?

**Framework:**  Construct 3 is based on a conceptual framework for professional development adapted by one by Amundsen et al. (2005) that categorizes four main focal areas in faculty development: skills focus, method focus, process focus, and discipline focus{Amundsen:NBNNckS3}.  The Amundsen framework was adapted for a disciplinary focus to the following framework for professional development activities. 

| Categories                                                                      | Disciplinary Focus                                                                                                                                                                                                                                                                                                                                                           | Multidisciplinary Focus                                                                                                                                                                                                                                                                         |
|---------------------------------------------------------------------------------|------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
| Skills (presentations, discussion facilitation, learning technology)            | Training on using engineering hardware/software in courses.  Personal reading on using engineering hardware/software in courses.                                                                                                                                                                                                                                             | Training on using general educational software/hardware (learning management systems, clickers, etc.)  Personal reading on general education hardware/software in courses (learning management systems, clickers, etc.)  Training on organization, presentations, writing on a blackboard, etc. |
| Teaching methods (project-based learning, case studies, active learning, etc.)  | Workshops/training on teaching methods specific to engineering (education sessions at disciplinary conferences, workshops on teaching design, engineering labs, etc.)  Personal reading on teaching methods specific to engineering (education sessions at disciplinary conferences, workshops on teaching design, engineering labs, etc.)                                   | Workshops/training on general teaching methods (active learning, service learning, collaborative learning, etc.)  Personal reading on general teaching methods (active learning, service learning, collaborative learning, etc.)                                                                |
| Processes and critical analysis                                                 | Workshops/sessions on teaching and learning processes specific to engineering (course redevelopment workshops, curriculum design, assessment, graduate attributes)  Facilitated sessions on working collaboratively as an engineering department on curriculum design, assessment, etc.  Broad informal holistic discussions on teaching and learning issues with colleagues | Workshop/session on general teaching and learning processes (constructive alignment, curriculum design, assessment, etc.) Facilitated sessions on curriculum design, assessment, learning science, etc.                                                                                         |
| Personal scholarship                                                            | Presenting and seeking feedback to engineering colleagues on teaching and learning innovations  Scholarly work related to engineering education                                                                                                                                                                                                                              |                                                                                                                                                                                                                                                                                                 |

##2.3 Phase 3


###2.3.1 DRAFT SURVEY

Once the frameworks were finalized, each working group created draft items for their respective construct.  These items were then reviewed by the steering committee alongside educational experts in survey creation.  The intent of the internal review was the streamline the draft survey prior to presenting it to collaborators at the CEEA 2014 meeting. This resulted in the removal and merging items, reallocating items to another construct, or outright removal of items that were deemed unnecessary.   The survey was then created in a web-based platform, FluidSurveys, as this was the survey platform used by the host institution, Queenâ€™s University.   This draft survey was packaged with a brief synopsis of the research collaboration thus far (Goals, Development, Research Questions and Frameworks) and was presented to collaborators at the 2014 annual meeting.

###2.2.2 ETHICS APPROVAL

In anticipation of questions from the community regarding ethics approval for the project, a draft application was created along with a combined letter of information and consent form.  The draft application was completed by the steering committee and would be submitted to the General Research Ethics Board (GREB) at Queenâ€™s University upon the survey being finalized.  Additionally, the steering committee consulted with the director of the Queenâ€™s Research Ethics Board (REB) for the proposed workflow for conducting collaborative national level research on this scale, shown below:

1. Steering committee applies to Queenâ€™s GREB for approval for the national study

2. Upon obtaining approval from Queenâ€™s apply to participating institutions individual REB boards, providing them the Queenâ€™s REB Approval, and all required supporting information

3. Once institutional approval is obtained, research can begin.

The merged letter of information and consent form was drafted according to Tri-Council Policy Statement (TCPS) and Queen's University guidelines.  These documents were prepared to give collaborators at other institutions an idea of what type of information would be required in their own future applications to their institutional REBs.  In order to meet the needs of specific institutional REBs, these draft approval documents would most likely be ammended.  

##2.4 FINALIZING THE SURVEY INSTRUMENT

Immediate collaborator and community feedback was collected from the session at the conclusion of the 2014 CEEA annual meeting.  Collaborators were also encouraged to take some time to consider and reflect upon the draft version of the survey, and provide their feedback to the steering committee by the end of the summer.

The majority of the feedback collected from the conference session was incredibly positiv and largely constituted corrections to tenses and phrasing about the instrument.  This resulted in some response options being changed, as collaborators thought some of the response options could be more clearly worded.  There were also some minor changes made to the demographics section of the survey, to provide better options and avenues for future analysis.  This feedback, along with the combined LOI and consent form was incorporated into a web-based survey in FluidSurveys.

###2.4.1 ETHICS APPROVAL

Upon completion of the final version of the survey, a formal application for the project was submitted to Queen's University.  The project received Queen's Ethics Approval September 10, 2014, nearly a full year after the project had started.  Due to the start of the academic year, the call for implementing the survey went out to collaborators on October 28th.  This message presented two options to deploy the survey, with each requiring that individual institutions obtain approval from their REB prior to starting research.  In order to support those new to this aspect of research, and to streamline individual applications, a member of the steering committee would work with collaborators to complete their respective ethics applications.

1. **Institutionally Hosted**: Institutions host and run the survey themsleves. They collect their survey data, then share a copy of with the steering committee for use in the aggregate national dataset.  

2. **Queen's Hosted**: Queen's hosts the survey, with each participating institution having it's own separate collector with unique variables to highlight differences in personnel and variations in addressing REB requests. Institutions are then provided a copy of their data for their own analysis.

The typical workflow for deployment combined choosing deployment options, ethics approval, getting approval from the Dean of Engineering, contacting faculty, and surveying.  For clarity, it is shown below:

1.  Select deployment strategy
2.  Contact ethics board to determine application type
3.  Work with steering committee member to obtain ethics approval
4.  Contact the Dean of Engineering to inform them and obtain approval
5.  Contact the faculty members to invite them to participate
6.  Schedule a reminder to participate at a later date

Institutional deployments started in late November of 2014, working towards obtaining institutional ethics approval.  The survey started to go live in January 2015.  The steering committee decided to let each institution set its own timeline, as many collaborators wished to coincide the deployment of the survey with specific events, or to adapt to unforeseen consequences.  

##2.5 CURRENT STATUS

As of writing this paper, there have been 10 different institutions that have been involved with the project thus far. The survey has launched and completed at 8 of those institutions, one of those institutions is waiting to contact faculty at the conclusion of the academic year, and one institution is just beginning the ethics approval process.  

The steering committee expects that following the presentation of this paper, there may be more institutions wanting to participate in the project.  To accomodate those yet to survey their faculty, and to accomodate potential newcomers we are keeping the project open until September 2015, which coincides with the ethics renewal for the project.

##3.0 ANALYSIS

The analysis presented in this paper is just a brief overview presenting summary statistics on the data collected up until April 15th, 2015.  

###3.1 DEMOGRAPHICS

The response for the survey was quite positive, illustrated below in table 1.

```{r Summary}
set.caption('1: Summary response statistics') 
survey_data %>% 
  summarise(Responses=n_distinct(key),
            Institutions=n_distinct(collector),
            "Avg.Time (mins)"=mean(time)) %>% 
  pander
```

Out of 22 institutions that expressed interest in the research collaboration, only 8 have implemented the survey.  At the April 15th cutoff point, two instituions were in the process of approval or implemenetation. If faculty counts were obtained for each institution, a true response rate could be genreated.  Respondents took an average time of 15.44 minutes to complete the survey, which is under the anecdotal 20 minute 'rule of thumb' to maintain survey engagement.

The distribution of instructor classification, shown below in table 2, illustrates that respondents were mostly classified as a traditional professor.  Those who responded "other", show a mix of Emeritus professors, adminstrators and specialized staff shown in table 3.

```{r Q1}
set.caption('2: Distribution of Instructor Classification') 
survey_data %>% 
  count("Academic Title"=position) %>% 
  pander
```

```{r Q1b}
set.caption('3: Other Roles') 
survey_data %>% 
  count("Academic Title"=position_other) %>%
  na.omit %>% 
  pander
```

The majority of the population was in the 7-15 years, with a relatively even distribution in the remaining categories, shown below in table 4. 
```{r Q2}
set.caption('4: Teaching Experience') 
survey_data %>% 
 count("Teaching Experience"=teaching_duration) %>% 
 pander
```

The majority of the population was focused largely on undergraduate teaching, reporting that teaching occupies approximately 21-60% of faculty work time.

```{r Q3&4,echo=FALSE}
u.vs.g <- survey_data %>% 
  count(undergrad_vs_grad) %>% 
  set_names(c('Category','# of Responses:Undergrad vs Graduate Teaching'))

avg.t.time <- survey_data %>% 
  count(avg_teaching_time) %>% 
  set_names(c('Category','# of Responses: Average teaching time per year'))

set.caption('5: Teaching focus and Division of Work Time') 
inner_join(u.vs.g,avg.t.time,by='Category') %>% 
  set_names(c('Percentage','# of Responses:Undergrad vs Graduate Teaching','# of Responses: Average teaching time per year')) %>% 
  pander
```

### 3.2 CONSTRUCT 1: GENERAL PERSPECTIVES AND ATTIUDES

The majority of survey respondents would classify themselves as enjoying teaching to a great or fairly great extent, as illustrated in figure 1 below.

```{r Q5}
survey_data %>% 
  ggplot(aes(x=enjoyment)) %>% 
  + geom_histogram( fill="#5AB4AC") %>% 
  + ggtitle("To what extent do you enjoy teaching?") %>% 
  + scale_x_discrete(name="") %>% 
  + theme(axis.text.x = element_text(angle=90, vjust=0.5, hjust=1)) %>% 
  + theme_hc() %>% 
  + scei.theme
```

The top teaching goals of survey respondents were transmission of information (76%, ranked by top priority), adopting a learner-centered development of understanding and facilitating construction of meaning (60%).  Nurturing students to reach their personal potential, was third (35%) with a relatively even split.  The lowest priorities were social change through education of the next generation of engineers (73% ranked lowest priority), apprenticeship by socializing students into the practice of engineering (55%).

```{r Q6}
survey_data %>%  
  select(c(11:15)) %>% 
  plyr::rename(c(goals_transmission = "Transmission",
           goals_apprenticeship = "Apprenticeship",
           goals_learner_centered = "Learner Centered",
           goals_nuturing = "Nurturing",
           goals_social_change = "Social Change")) %>% 
  likert() %>% 
  plot() %>% 
  + ggtitle('Teaching Goals of Engineering Faculty') %>% 
  + scei.theme %>% 
  + theme(legend.text= element_text(size=6))
```

The primary influences of survey respondents to make changes in their teaching are oultined below in figure 3.  Personal obersvation from previous teaching (92%) and course eveluations (63%) were the mostinfluential, with professional development (31%), literature on teaching and learning (29%) and input from colleagues being indicated as being less influential.

```{r Q7}
survey_data %>%  
  select(c(18:22)) %>% 
  plyr::rename(c(changes_personal_observation = "Personal Observation",
           changes_colleague_input = "Colleague Input",
           changes_course_evaluations = "Course Evaluations",
           changes_literature = "Literature",
           changes_pd = "Professional Development")) %>% 
  likert() %>% 
  plot() %>% 
  + ggtitle('Influences to change teaching') %>% 
  + scei.theme %>% 
  + theme(legend.text= element_text(size=6))
```

```{r Q8}
q.8 <- survey_data %>% 
  select(c(25:37)) %>% 
  colSums(na.rm=TRUE) %>% 
  data.frame(names(.),.)

rownames(q.8) <- NULL

q.8 %<>%
  setNames(c("Reason","Frequency"))

q.8$Reason %<>%
 str_replace_all("_"," ") %>% 
 stri_trans_totitle()

set.caption('6: Other Roles') 
q.8 %>% 
  pander
```


```{r Q10}
survey_data %>% 
  select(c(39:41)) %>% 
  plyr::rename(c(instructor_teaching_practises = "Responsible to know &\n follow best teaching practices",
           instructor_content = "Responsible to know content &\n articulate it well.",
           instructor_motivate_and_provide = "Responsible to motivate students,\n explain learning &\n provide engaging learning opportunities.")) %>% 
  likert() %>% 
  plot() %>% 
  + ggtitle('Perception of the responsibility\n of the instructor in the\n teaching and learning process') %>% 
  + scei.theme %>% 
  + theme(legend.text= element_text(size=6))
```

```{r Q11}
survey_data %>% 
  select(c(42:44)) %>% 
  plyr::rename(c(student_ability_no_responsbility = "If the student has the ability to do the work,\n they doesnâ€™t really need to take on any added responsibility.",
           student_responsible_background_motivation = "The student is responsible for background knowledge and motivation",
           student_attend_classes = "The student is responsible to attend classes, be attentive,\n take good notes, do the readings and the assignments, and study.")) %>% 
  likert %>% 
  plot %>% 
  + ggtitle('Perception of the responsibility\n of the of the student in the\n teaching and learning process') %>% 
  + scei.theme %>% 
  + theme(legend.text= element_text(size=6))
```

```{r Q12}
survey_data %>% 
  select(c(45:47)) %>% 
  plyr::rename(c(methods_supporting_outcomes = "Use teaching and assessment methods that support clearly stated learning outcomes.",
           presentation_opportunity_management = "Speak effectively, provide structured and engaging learning opportunities, and manage the classroom effectively.",
           content_clarity = "Know the subject very well and explain it very clearly.")) %>% 
  likert %>% 
  plot %>% 
  + ggtitle('Perception of how instructors can positively influence student success') %>% 
  + scei.theme %>% 
  + theme(legend.text= element_text(size=6))
```

```{r Q14&15}
survey_data %>% 
  select(c(49:50)) %>% 
  plyr::rename(c(knowledge_expert = "I think of myself as a knowledge expert in my area of specialization.",
           teaching_expert = "I think of myself as an expert in teaching my area of specialization")) %>% 
  likert %>% 
  plot %>% 
  + ggtitle('Perception of instructor expertise in disciplinary content knowledge and teaching') %>% 
  + scei.theme %>% 
  + theme(legend.text= element_text(size=6))
```

```{r Q16}
q.16 <- survey_data %>% 
  select(c(51)) %>% 
  table() %>% 
  as.data.frame %>% 
  setNames(c("Action","Frequency"))

set.caption('7: Perception of important uses of student course & teaching evaluations') 
q.16 %>% 
  pander
```

```{r Q18}
survey_data %>% 
  select(c(53:56)) %>% 
  plyr::rename(c(teaching_skills_development = "Teaching Skills Development",
           teaching_assessment_methods = "Teaching Assessment Methods",
           teaching_processes = "Teaching Processes",
           scholarship = "Scholarhip")) %>% 
  likert %>% 
  plot %>% 
  + ggtitle('Perception of the importance of select professional development activites') %>% 
  + scei.theme %>% 
  + theme(legend.text= element_text(size=6))
```

```{r Q19}
survey_data %>% 
  select(c(57:60)) %>% 
  plyr::rename(c(support_teaching_skills_development = "Support for Teaching Skills Development",
           support_teaching_assessment_methods = "Support for Teaching Assessment Methods",
           support_teaching_processes = "Support for Teaching Processes",
           support_scholarship = "Support for Scholarhip")) %>% 
  likert %>% 
  plot %>% 
  + ggtitle('Instructor awarness and participation in professional development activities') %>% 
  + scei.theme %>% 
  + theme(legend.text= element_text(size=6))
```

```{r Q20}
q.20 <- survey_data %>% 
  select(c(61:70)) %>% 
  colSums(na.rm = TRUE) %>% 
  data.frame(names(.),.)

rownames(q.20) <- NULL

q.20 %<>%
  setNames(c("Activity","Frequency"))

q.20$Activity <- c("Attended a seminar on teaching (1-2 hours of professional development)",
                   "Participated in a workshop on teaching  (3 hours to a full day of professional development)",
                  "Participated in a multi-day workshop on teaching (several day professional development activity)",
                  "Participated in conference related to education (either disciplinary or not)",
                  "Learning independently through reading, etc.",
                  "Led workshops focusing on teaching and learning development",
                  "Internal university funding to support course or program development",
                  "External funding to support course or program development",
                  "Internal university grants supporting educational research",
                  "External grants supporting educational research)")

set.caption('8: Instructor participation in faculty development activities in the past 5 years') 
q.20 %>% 
  pander
```

```{r Q21}
survey_data %>% 
  select(c(71:79)) %>% 
  plyr::rename(c(obs_timing = "Timing of event",
                 obs_availability = "Availability of event",
                 obs_location = "Location of event",
                 obs_awareness = "Awareness of event",
                 obs_relevance = "Relevance of event",
                 obs_workload = "Workload",
                 obs_lack_funding = "Lack of funding opportunities",
                 obs_lack_expertise = "Lack of access to expertise",
                 obs_specificity = "General vs discipline specific nature of the event")) %>% 
  likert() %>% 
  plot %>% 
  + ggtitle('Instructor perception of obstacles to professional development') %>% 
  + scei.theme %>% 
  + theme(legend.text= element_text(size=6))
```


```{r Q22}
set.caption('9: Percentage of teaching-focused professional development activities attended in the past five years ') 
survey_data %>% 
  select(c(80)) %>% 
  table() %>% 
  as.data.frame %>% 
  setNames(c("Percentage","Frequency")) %>% 
  pander
```


```{r Q23a}
survey_data %>% 
  select(c(81:84)) %>% 
  plyr::rename(c(nf_teaching_skills = "Teaching skills development",
                 nf_teaching_assessment_methods = "Teaching of assessment methods",
                 nf_teaching_processes = "Teaching processes",
                 nf_scholarship = "Scholarship")) %>% 
  likert() %>% 
  plot %>% 
  + ggtitle('Instructor perception of needed areas of professional development as training for new faculty') %>% 
  + scei.theme %>% 
  + theme(legend.text= element_text(size=6))
```

```{r Q23b}
survey_data %>% 
  select(c(85:88)) %>% 
  plyr::rename(c(ce_teaching_skills = "Teaching skills development",
                 ce_teaching_assessment_methods = "Teaching of assessment methods",
                 ce_teaching_processes = "Teaching processes",
                 ce_scholarship = "Scholarship")) %>% 
  likert() %>% 
  plot %>% 
  + ggtitle('Instructor perception of needed areas of professional development as continuing education for for experienced faculty') %>% 
  + scei.theme %>% 
  + theme(legend.text= element_text(size=6))

```

```{r Q24}
set.caption('10: Instructor perception of institutions taking professional development activities into account during annual performance evaluations ') 
survey_data %>% 
  select(c(89)) %>% 
  table() %>% 
  as.data.frame %>% 
  setNames(c("Percentage","Frequency")) %>% 
  pander
```